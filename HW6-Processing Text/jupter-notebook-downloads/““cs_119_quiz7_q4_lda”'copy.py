# -*- coding: utf-8 -*-
"""““cs-119-quiz7-q4-lda”'copy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15NLSwOkpNNIeSMYIHw7nbqS5TYLclMa-

$\color{blue}{\text{cs-119 Quiz7 q4 (LDA)}}$

The first 8 cells of this notebook install PySpark. The quiz questions follow.
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -qN https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-3.2.1-bin-hadoop3.2"

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('Reviews LDA') \
                    .getOrCreate()

sc = spark.sparkContext

from pyspark.sql.types import *

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

import pandas as pd
import pyspark
from pyspark.sql import SQLContext

"""$\color{blue}{\text{Q1: Read the reviews data from the source and scrub it}}$

The data source is [here](https://storage.googleapis.com/jsingh-bigdata-public/online-retail/reviews.csv). 

Download it to a Pandas DataFrameClean the text in the Title and Review Text columns according to these rules:

1. Remove all punctuations,
2. Turn all words into lowercase,
3. Reject all 3-character or smaller words.
"""

df = pd.read_csv('reviews.csv')
df.dropna(inplace=True)
for (columnName, columnData) in df.iteritems():
  if pd.api.types.is_string_dtype(df[columnName].dtype):
    df[columnName] = df[columnName].str.lower()
    df[columnName]=df[columnName].str.findall('\w{3,}').str.join(' ')
    df[columnName] = df[columnName].str.replace(r'[^\w\s]+', '')
df

"""Each review is organized as a row in the PySpark DataFrame, and the objective is to do the same processing on each review in parallel!

$\color{blue}{\text{Q2: Convert the Pandas DataFrame into a PySpark DataFrame}}$

"""

sparkDF = spark.createDataFrame(df)
sparkDF.printSchema()
sparkDF.show()

"""$\color{blue}{\text{Q3: Tokenize Review Text}}$

NLTK provides its own stop words. Using these has the advantage that we can use NLTK-provided stop words for a variety of supported languages. 

This is an excellent place to further process the text. A tokenizer for the Review Text is provided for you here, you may use it as-is or modify it as you see fit.

```
def tokenize(pyspark_DataFrame):
    import re
    from nltk.corpus import stopwords
    reviews = pyspark_DataFrame.rdd.map(lambda x : x['Review Text'])   \
        .filter(lambda x: x is not None)
    StopWords = stopwords.words("english")
    tokens = reviews                                                   \
        .map( lambda doc: doc.strip().lower())                         \
        .map( lambda doc: re.split(" ", doc))                          \
        .map( lambda word: [x for x in word if x.isalpha()])           \
        .map( lambda word: [x for x in word if x not in StopWords])    \
        .zipWithIndex()
    return tokens
```


"""

def tokenize(pyspark_DataFrame):
    import re
    from nltk.corpus import stopwords
    reviews = pyspark_DataFrame.rdd.map(lambda x : x['Review Text'])   \
        .filter(lambda x: x is not None)
    StopWords = stopwords.words("english")
    tokens = reviews                                                   \
        .map( lambda doc: doc.strip().lower())                         \
        .map( lambda doc: re.split(" ", doc))                          \
        .map( lambda word: [x for x in word if x.isalpha()])           \
        .map( lambda word: [x for x in word if x not in StopWords])    \
        .zipWithIndex()
    return tokens

token = tokenize(sparkDF)
sparkDF.show()

"""$\color{blue}{\text{Q4: TF.IDF Calculation}}$

Feed the resulting tokens into a TF.IDF calculation. TF calculation is provided by `CountVectorizer`, and IDF calculation by `IDF`, both are available in the `pyspark.ml.feature` library.
```
def tfidf(sc, tokens):
    from pyspark.sql import SQLContext
    from pyspark.ml.feature import CountVectorizer , IDF
    sqlContext = SQLContext(sc)
    df_txts = sqlContext.createDataFrame(tokens, ["list_of_words",'index'])
    #
    # TF
    #
    cv = CountVectorizer(inputCol="list_of_words", outputCol="raw_features", \
        vocabSize=5000, minDF=10.0)
    cvmodel = cv.fit(df_txts)
    result_cv = cvmodel.transform(df_txts)
    #
    # IDF
    #
    idf = IDF(inputCol="raw_features", outputCol="features")
    idfModel = idf.fit(result_cv)
    tfidf_result = idfModel.transform(result_cv)
    #
    return tfidf_result
```
"""

def tfidf(sc, tokens):
    from pyspark.sql import SQLContext
    from pyspark.ml.feature import CountVectorizer , IDF
    sqlContext = SQLContext(sc)
    df_txts = sqlContext.createDataFrame(tokens, ["list_of_words",'index'])
    #
    # TF
    #
    cv = CountVectorizer(inputCol="list_of_words", outputCol="raw_features", \
        vocabSize=5000, minDF=10.0)
    cvmodel = cv.fit(df_txts)
    result_cv = cvmodel.transform(df_txts)
    #
    # IDF
    #
    idf = IDF(inputCol="raw_features", outputCol="features")
    idfModel = idf.fit(result_cv)
    tfidf_result = idfModel.transform(result_cv)
    #
    return tfidf_result, cvmodel

tf_res, cvmodel = tfidf(sc, tokenize(sparkDF))

"""$\color{blue}{\text{Q5: LDA Training}}$

The TF.IDF calculations form the input into LDA. An `lda_train()` function (shown below) takes columns `index` and `features` from the `tfidf` DataFrame and calculates the LDA Model. This calculation is referred to as *Training* the model.

```
def lda_train(result_tfidf):
    from pyspark.ml.linalg import Vectors, SparseVector
    from pyspark.ml.clustering import LDA
    #
    lda = LDA(k=10, seed=1, optimizer="em")
    lda.setMaxIter(100)
    #
    model = lda.fit(result_tfidf[['index', 'features']])
    return model

```
With the reviews dataset, LDA training takes about 15-20 minutes in a Colab environment. 


"""

def lda_train(result_tfidf):
    from pyspark.ml.linalg import Vectors, SparseVector
    from pyspark.ml.clustering import LDA
    #
    lda = LDA(k=10, seed=1, optimizer="em")
    lda.setMaxIter(100)
    #
    model = lda.fit(result_tfidf[['index', 'features']])
    return model

"""#### This will take about 15 minutes"""

model = lda_train(tf_res)

newDF = model.describeTopics(10)
newDF.show()

"""$\color{blue}{\text{Q6: Reporting on the LDA Model}}$

Examine the model generated during training. It will show the 10 topics it generated.

What is a topic? Just a list of words that "hang together." Does the collection of words describe a topic to you? What might it be?
"""

def getDescrib(input, voca):
  res = []
  for i in input:
      res.append(voca[i])
  return res

voca = cvmodel.vocabulary

pandaDF = newDF.toPandas()
descrip = []
for i in range(pandaDF.shape[0]):
  input = pandaDF.at[i, 'termIndices']
  temp = getDescrib(input, voca)
  descrip.append(temp)

pandaDF['description'] = descrip
sparkDF1 = spark.createDataFrame(pandaDF)
sparkDF1.printSchema()
sparkDF1.show()
res = sparkDF1.toPandas()
res

"""I think most of them tells me something(I have to change the df to pandas so that I can see full content of description:

1. feels like someone its complaining that the small size thing runs loose to large size 

2. someone is happy with the black jeans he/she bought

3. someone bought a shirt that looks like the picture online, positive feedback

4. someone like cloth he bought and it is his first time

5. can't tell if this is a positive or nagtive review but it is about a blouse

6. looks someone bought a small size pants

7. someone is happy with the skir it bought

8. can hardly tell what it says too many words

9. someone really like the thing he bought maybe about the style and matrerial

10. someone bought a sweater and jacket with light green color
"""